{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8125fcc-fd8b-4788-aed8-94c529db27f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU count: 2\n",
      "‚úÖ Paths OK.\n"
     ]
    }
   ],
   "source": [
    "import os, gc, torch\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU count:\", torch.cuda.device_count())\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# ---- Paths (igual que tu script) ----\n",
    "BASE_MODEL_DIR = \"./local_llama2_model\"          # snapshot_download\n",
    "ADAPTER_DIR    = \"llama2_7b_unt_lora_rag\"        # OUTPUT_DIR del entrenamiento\n",
    "FAISS_DIR      = \"faiss_unt_index_llama2\"        # index guardado\n",
    "CSV_PATH       = \"qa_dataset.csv\"                # dataset original\n",
    "\n",
    "assert os.path.isdir(BASE_MODEL_DIR), \"Base model dir not found\"\n",
    "assert os.path.isdir(ADAPTER_DIR),    \"Adapter dir not found\"\n",
    "assert os.path.isdir(FAISS_DIR),      \"FAISS dir not found\"\n",
    "assert os.path.isfile(CSV_PATH),      \"CSV not found\"\n",
    "\n",
    "print(\"‚úÖ Paths OK.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b38b72d7-50db-4aa4-a081-7b89b6a750c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 601\n",
      "Test rows: 61\n",
      "{'question': '¬øQu√© es el OPT y qui√©n califica para aplicarlo?', 'context': 'OPT (Optional Practical Training) es una autorizaci√≥n de trabajo temporal para estudiantes con visa F-1 que les permite obtener experiencia laboral en su campo de estudio. Para calificar, debes haber estado inscrito a tiempo completo en una universidad de EE.UU. por al menos un a√±o acad√©mico completo y tener un estatus de visa F-1 v√°lido.', 'answer': 'El OPT es un permiso de trabajo temporal para estudiantes F-1. Para calificar, debes haber estudiado a tiempo completo por al menos un a√±o acad√©mico y tener un estatus de visa v√°lido.'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "df = pd.read_csv(CSV_PATH).dropna(subset=[\"question\",\"context\",\"answer\"]).reset_index(drop=True)\n",
    "ds = Dataset.from_pandas(df)\n",
    "\n",
    "split = ds.train_test_split(test_size=0.1, seed=SEED)\n",
    "test_raw = split[\"test\"]\n",
    "\n",
    "print(\"Total rows:\", len(df))\n",
    "print(\"Test rows:\", len(test_raw))\n",
    "print(test_raw[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8f8d281-0711-49fd-b303-4b76dde5a7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_618725/1061191145.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "/home/STUDENTS/hel0057/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FAISS loaded.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    model_kwargs={\"device\": \"cpu\"}  # retrieval estable\n",
    ")\n",
    "\n",
    "vectorstore = FAISS.load_local(FAISS_DIR, embeddings, allow_dangerous_deserialization=True)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "print(\"‚úÖ FAISS loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9b25bee-1987-440e-97b0-250209e6c105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f3542856b02400182c8b9e0f67da69e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded on: cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "def load_llm_with_fallback():\n",
    "    tok = AutoTokenizer.from_pretrained(ADAPTER_DIR, use_fast=True)\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "\n",
    "    # Try GPU first\n",
    "    if torch.cuda.is_available():\n",
    "        try:\n",
    "            dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "            base = AutoModelForCausalLM.from_pretrained(\n",
    "                BASE_MODEL_DIR,\n",
    "                torch_dtype=dtype,\n",
    "                device_map=\"auto\",\n",
    "                low_cpu_mem_usage=True\n",
    "            )\n",
    "            base.resize_token_embeddings(len(tok))\n",
    "            base.config.use_cache = True\n",
    "\n",
    "            ft = PeftModel.from_pretrained(base, ADAPTER_DIR)\n",
    "            ft.eval()\n",
    "            return tok, ft, \"cuda\"\n",
    "        except Exception as e:\n",
    "            print(\"‚ö†Ô∏è GPU load failed, falling back to CPU.\")\n",
    "            print(\"Reason:\", repr(e))\n",
    "\n",
    "    # CPU fallback\n",
    "    dtype = torch.float32\n",
    "    base = AutoModelForCausalLM.from_pretrained(BASE_MODEL_DIR, torch_dtype=dtype)\n",
    "    base.resize_token_embeddings(len(tok))\n",
    "    base.to(\"cpu\")\n",
    "    base.config.use_cache = True\n",
    "\n",
    "    ft = PeftModel.from_pretrained(base, ADAPTER_DIR)\n",
    "    ft.to(\"cpu\")\n",
    "    ft.eval()\n",
    "    return tok, ft, \"cpu\"\n",
    "\n",
    "tokenizer, model, MODEL_DEVICE = load_llm_with_fallback()\n",
    "print(\"‚úÖ Model loaded on:\", MODEL_DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1950d824-78b2-4bab-a19b-59b64dfeeade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generation ready (token-slicing + stop + postprocess).\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import List, Optional\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# --------------------------\n",
    "# 1) Language helpers\n",
    "# --------------------------\n",
    "\n",
    "def detect_forced_language(q: str):\n",
    "    ql = q.lower().strip()\n",
    "    if re.search(r\"\\b(answer|respond)\\s+in\\s+english\\b\", ql) or \"answer in english:\" in ql:\n",
    "        return \"en\"\n",
    "    if re.search(r\"\\b(answer|respond)\\s+in\\s+spanish\\b\", ql) or \"answer in spanish:\" in ql or \"en espa√±ol\" in ql or \"en espanol\" in ql:\n",
    "        return \"es\"\n",
    "    return None\n",
    "\n",
    "def guess_language(q: str):\n",
    "    q = q.strip()\n",
    "    ql = q.lower()\n",
    "\n",
    "    if q.startswith((\"¬ø\", \"¬°\")) or any(ch in ql for ch in \"√°√©√≠√≥√∫√±√º\"):\n",
    "        return \"es\"\n",
    "\n",
    "    es_hits = sum(w in ql.split() for w in [\n",
    "        \"que\",\"qu√©\",\"como\",\"c√≥mo\",\"para\",\"requisitos\",\"documentos\",\n",
    "        \"proceso\",\"visa\",\"i-20\",\"sevis\"\n",
    "    ])\n",
    "    en_hits = sum(w in ql.split() for w in [\n",
    "        \"what\",\"how\",\"requirements\",\"documents\",\n",
    "        \"process\",\"apply\",\"visa\",\"i-20\",\"sevis\"\n",
    "    ])\n",
    "\n",
    "    # ‚úÖ INGL√âS POR DEFECTO\n",
    "    return \"es\" if es_hits > en_hits else \"en\"\n",
    "\n",
    "def target_language(q: str):\n",
    "    forced = detect_forced_language(q)\n",
    "    return forced if forced else guess_language(q)\n",
    "\n",
    "# --------------------------\n",
    "# 2) Postprocess: limpia markers\n",
    "# --------------------------\n",
    "def postprocess_answer(ans: str) -> str:\n",
    "    a = ans.strip()\n",
    "\n",
    "    junk_markers = [\n",
    "        \"[USER QUESTION]\", \"[FINAL ANSWER]\", \"[RETRIEVED CONTEXT]\",\n",
    "        \"[TARGET_LANGUAGE=\", \"TARGET_LANGUAGE=\", \"Sources:\", \"SOURCES:\"\n",
    "    ]\n",
    "\n",
    "    lines = a.splitlines()\n",
    "    clean_lines = []\n",
    "    for line in lines:\n",
    "        l = line.strip()\n",
    "        low = l.lower()\n",
    "        if any(m.lower() in low for m in junk_markers):\n",
    "            continue\n",
    "        clean_lines.append(line)\n",
    "\n",
    "    a = \"\\n\".join(clean_lines).strip()\n",
    "    a = re.sub(r\"(?i)^\\s*(final answer|respuesta final)\\s*:\\s*\", \"\", a).strip()\n",
    "    a = re.sub(r\"(?i)^\\s*\\[target_language\\s*=\\s*(en|es)\\s*\\]\\s*\", \"\", a).strip()\n",
    "    a = re.sub(r\"\\n{3,}\", \"\\n\\n\", a).strip()\n",
    "    return a\n",
    "\n",
    "def hard_clean_and_cut(ans: str, lang: str) -> str:\n",
    "    a = ans.strip()\n",
    "\n",
    "    # 1) elimina tokens/labels basura\n",
    "    a = re.sub(r\"(?im)^\\s*\\[answer\\]\\s*\", \"\", a).strip()\n",
    "    a = re.sub(r\"(?im)^\\s*(answer|final answer)\\s*:\\s*\", \"\", a).strip()\n",
    "\n",
    "    # 2) si el modelo intenta seguir con otro Q/A, corta ah√≠\n",
    "    cut_markers = [\n",
    "        \"\\nQuestion:\", \"\\nAnswer:\", \"\\nQ:\", \"\\nA:\",\n",
    "        \"\\nSources:\", \"\\nSOURCES:\",\n",
    "        \"\\nWhat is\", \"\\n¬øQu√© es\", \"\\nQu'est-ce\"\n",
    "    ]\n",
    "    for m in cut_markers:\n",
    "        if m in a:\n",
    "            a = a.split(m)[0].strip()\n",
    "\n",
    "    # 3) corta si aparecen m√∫ltiples p√°rrafos y el segundo parece otra respuesta\n",
    "    # (conservador: nos quedamos con el primer bloque)\n",
    "    blocks = [b.strip() for b in re.split(r\"\\n\\s*\\n\", a) if b.strip()]\n",
    "    if len(blocks) >= 2:\n",
    "        a = blocks[0].strip()\n",
    "\n",
    "    # 4) √∫ltimo toque: quita l√≠neas sueltas tipo \"Question\" sin :\n",
    "    a = re.sub(r\"(?im)^\\s*question\\s*$\", \"\", a).strip()\n",
    "    a = re.sub(r\"\\n{3,}\", \"\\n\\n\", a).strip()\n",
    "\n",
    "    return a\n",
    "\n",
    "# --------------------------\n",
    "# 3) Stopping criteria: corta si intenta imprimir markers\n",
    "# --------------------------\n",
    "class StopOnSubsequence(StoppingCriteria):\n",
    "    def __init__(self, stop_token_seqs: List[List[int]]):\n",
    "        super().__init__()\n",
    "        self.stop_token_seqs = stop_token_seqs\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        seq = input_ids[0].tolist()\n",
    "        for stop_seq in self.stop_token_seqs:\n",
    "            L = len(stop_seq)\n",
    "            if L > 0 and len(seq) >= L and seq[-L:] == stop_seq:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "STOP_STRINGS = [\n",
    "    \"[USER QUESTION]\", \"[FINAL ANSWER]\", \"[RETRIEVED CONTEXT]\",\n",
    "    \"\\nQuestion:\", \"\\nAnswer:\", \"\\nQ:\", \"\\nA:\",\n",
    "    \"\\n[ANSWER]\", \"\\nSources:\", \"\\nSOURCES:\",\n",
    "    \"\\nQu'est-ce\", \"\\n¬øQu√© es\", \"\\nWhat is\"\n",
    "]\n",
    "STOP_TOKEN_IDS = []\n",
    "for s in STOP_STRINGS:\n",
    "    ids = tokenizer.encode(s, add_special_tokens=False)\n",
    "    if len(ids) > 0:\n",
    "        STOP_TOKEN_IDS.append(ids)\n",
    "\n",
    "stopping = StopOnSubsequence(STOP_TOKEN_IDS)\n",
    "stopping_criteria = StoppingCriteriaList([stopping])\n",
    "\n",
    "# --------------------------\n",
    "# 4) Robust generation: slice by input token length\n",
    "# --------------------------\n",
    "@torch.no_grad()\n",
    "def generate_text(prompt: str, max_new_tokens: int = 180, temperature: float = 0.7, top_p: float = 0.9):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "\n",
    "    if MODEL_DEVICE == \"cuda\":\n",
    "        first_device = next(model.parameters()).device\n",
    "        inputs = {k: v.to(first_device) for k, v in inputs.items()}\n",
    "\n",
    "    input_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "    out = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs.get(\"attention_mask\", None),\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        repetition_penalty=1.05,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        stopping_criteria=stopping_criteria\n",
    "    )\n",
    "\n",
    "    gen_ids = out[0][input_len:]  # ‚úÖ SOLO lo nuevo\n",
    "    text = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "    return postprocess_answer(text)\n",
    "\n",
    "print(\"‚úÖ Generation ready (token-slicing + stop + postprocess).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1356be84-bd43-4697-a9f9-6200cf7eaedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAG ready.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "class LocalLLM(LLM):\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"local_llama2_rag\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None, **kwargs):\n",
    "        text = generate_text(prompt)\n",
    "        if stop:\n",
    "            for s in stop:\n",
    "                if s in text:\n",
    "                    text = text.split(s)[0]\n",
    "        return postprocess_answer(text)\n",
    "\n",
    "llm = LocalLLM()\n",
    "\n",
    "rag_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=(\n",
    "        \"You are a RAG assistant for international students.\\n\"\n",
    "        \"The user's question starts with a language tag: [LANG=en] or [LANG=es].\\n\\n\"\n",
    "        \"STRICT OUTPUT RULES (must follow):\\n\"\n",
    "        \"1) Respond ONLY in the language specified by the tag.\\n\"\n",
    "        \"2) Output ONLY the final answer text. No labels, no markup.\\n\"\n",
    "        \"3) DO NOT write 'Question:', 'Answer:', 'Q:', '[ANSWER]', or any extra sections.\\n\"\n",
    "        \"4) DO NOT ask follow-up questions. DO NOT generate multiple Q&A pairs.\\n\"\n",
    "        \"5) If the context is insufficient, say you don't know and ask for the missing detail (same language).\\n\\n\"\n",
    "        \"Context:\\n{context}\\n\\n\"\n",
    "        \"User question:\\n{question}\\n\\n\"\n",
    "        \"Final answer (one paragraph):\"\n",
    "    )\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": rag_prompt},\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "def chat_rag(q: str, k: int = 3):\n",
    "    qa_chain.retriever.search_kwargs[\"k\"] = k\n",
    "\n",
    "    # idioma (EN por defecto, ES si hay se√±ales claras o force)\n",
    "    lang = target_language(q)\n",
    "\n",
    "    # limpia \"Answer in English:\" si lo usan\n",
    "    q_clean = re.sub(r\"(?i)^\\s*answer in (english|spanish)\\s*:\\s*\", \"\", q).strip()\n",
    "\n",
    "    # üëá el tag viaja dentro del texto, sin cambiar inputs del chain\n",
    "    tagged_q = f\"[LANG={lang}] {q_clean}\"\n",
    "\n",
    "    res = qa_chain({\"query\": tagged_q})\n",
    "    ans = postprocess_answer(res[\"result\"])\n",
    "    ans = hard_clean_and_cut(ans, lang)\n",
    "\n",
    "    return ans\n",
    "\n",
    "print(\"‚úÖ RAG ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c322b36d-0c5e-46d4-b31a-fee11ee979ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ RAG Chat ready. Type 'exit' to stop.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Que es SEVIS?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bot: SEVIS es un sistema inform√°tico que registra y monitorea las visitas de estudiantes internacionales en Estados Unidos. Es necesario para mantener la informaci√≥n de los estudiantes extranjeros y asegurar que cumplan con las leyes federales y universitarias. Los estudiantes internacionales deben completar el formulario SEVIS I-901 antes de llegar a Estados Unidos y actualizarlo cada a√±o.\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  What is SEVIS?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bot: SEVIS is a government system that tracks international students. Your university must keep your information updated in it for your visa status to remain valid.\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  WHat is the I-20?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bot: The I-20 form is issued by the university and is required to pay the SEVIS fee and apply for your F-1 student visa. It confirms your acceptance and legal eligibility for a student visa.\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Que es el I-20?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bot: El I-20 es un documento oficial emitido por tu universidad que demuestra tu admisi√≥n y tus fondos para estudiar en EE.UU. Es la prueba principal que necesitas para solicitar tu visa F-1 y entrar al pa√≠s. Contiene informaci√≥n importante sobre ti y el programa de estudios, incluyendo tu nombre, tu n√∫mero de SEVIS, el nombre de la universidad, tu fecha de inicio y finalizaci√≥n del programa, y una estimaci√≥n de los costos de matr√≠cula y de vida.\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  How do I apply to a US university?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bot: The key steps are to research universities, prepare for standardized tests, write essays, gather documents, apply for a visa, and get ready for your arrival. It is a long process that requires advance planning.\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  COmo aplico a una universidad en los Estados Unidos?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bot: Para aplicar a una universidad en los Estados Unidos, debes cumplir con los requisitos m√≠nimos de ingreso, incluyendo el TOEFL o IELTS si provienes de un pa√≠s no de habla inglesa. Investigar las pol√≠ticas de ayuda financiera de cada universidad es crucial para encontrar opciones de financiaci√≥n.\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Bye üëã\n"
     ]
    }
   ],
   "source": [
    "def run_chat():\n",
    "    print(\"ü§ñ RAG Chat ready. Type 'exit' to stop.\\n\")\n",
    "    while True:\n",
    "        q = input(\"You: \").strip()\n",
    "        if q.lower() in {\"exit\", \"quit\", \"q\"}:\n",
    "            print(\"Bot: Bye üëã\")\n",
    "            break\n",
    "        ans = chat_rag(q, k=3)\n",
    "        print(\"\\nBot:\", ans)\n",
    "        print(\"\\n\" + \"-\"*60 + \"\\n\")\n",
    "\n",
    "run_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de36e211-fdc8-40ae-a6ee-5927bc82d8a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
