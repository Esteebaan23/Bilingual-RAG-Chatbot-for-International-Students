{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98b7818a-389d-466d-a162-3cff1fb44b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned. CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import os, gc, torch\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Cleaned. CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5971e4b-93ec-4d11-8cf2-72ee43b1dd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Paste your HF token (input hidden):  Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_TOKEN set: True\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Set HF token safely (do NOT print it)\n",
    "\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if \"HF_TOKEN\" not in os.environ:\n",
    "    os.environ[\"HF_TOKEN\"] = getpass(\"Paste your HF token (input hidden): \")\n",
    "\n",
    "print(\"HF_TOKEN set:\", \"HF_TOKEN\" in os.environ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "430383ee-2160-4571-a93a-e7d20fe176d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8404ef7df6ae4b0693a3693113b7f5d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 16 files:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded to: ./local_llama2_model\n"
     ]
    }
   ],
   "source": [
    "# Cell 2b: snapshot_download Llama-2-7b-chat-hf to local dir\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "LOCAL_MODEL_DIR = \"./local_llama2_model\"\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    local_dir=LOCAL_MODEL_DIR,\n",
    "    token=os.environ[\"HF_TOKEN\"],\n",
    "    local_dir_use_symlinks=False\n",
    ")\n",
    "\n",
    "print(\"Downloaded to:\", LOCAL_MODEL_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27c79dd9-bc39-49dc-899e-13ea96ac89e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x72f0207ef6d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 3: Config\n",
    "\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "BASE_MODEL = LOCAL_MODEL_DIR\n",
    "OUTPUT_DIR = \"llama2_7b_unt_lora_rag\"\n",
    "\n",
    "# Training params (agresivos para que quepa en 20GB)\n",
    "MAX_SEQ_LEN = 192\n",
    "BATCH_SIZE = 1\n",
    "GRAD_ACC = 32\n",
    "EPOCHS = 2\n",
    "LR = 2e-4\n",
    "SEED = 42\n",
    "\n",
    "torch.manual_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0af5c1f-da53-48d6-840b-dc6180b7f263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 601\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['question', 'context', 'answer'],\n",
       "     num_rows: 540\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['question', 'context', 'answer'],\n",
       "     num_rows: 61\n",
       " }))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 4: Load CSV and split\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "csv_path = \"qa_dataset.csv\"  # tu archivo\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "required = {\"question\", \"context\", \"answer\"}\n",
    "missing = required - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns: {missing}\")\n",
    "\n",
    "df = df.dropna(subset=[\"question\", \"context\", \"answer\"]).reset_index(drop=True)\n",
    "print(\"Rows:\", len(df))\n",
    "\n",
    "ds = Dataset.from_pandas(df)\n",
    "split = ds.train_test_split(test_size=0.1, seed=SEED)\n",
    "train_raw, test_raw = split[\"train\"], split[\"test\"]\n",
    "train_raw, test_raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9c9a538-822a-4dc1-ae65-b06cbfa6e2f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a592fdc93d94014a99190da1985ed37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/540 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a68fe9f12864213886baf6a326aef84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/61 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a professional assistant for international students at the University of North Texas (UNT).\n",
      "Always answer in the SAME language as the user's question (English question -> English answer, Spanish question -> Spanish answer).\n",
      "Be precise, specific, and factual. Use the context.\n",
      "\n",
      "[CONTEXT]\n",
      "If you have trouble, don't hesitate to ask for help. The International Student Office is your primary resource. They can give you advice on cultural adjustment, visa rules, or refer you to academic and mental health counselors on campus. Universities have many resources to help you succeed.\n",
      "\n",
      "[QUESTION]\n",
      "What should I do if I have trouble adapting?\n",
      "\n",
      "[ANSWER]\n",
      "If you're struggling, contact the Internationa\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Build SFT text field\n",
    "\n",
    "def build_text(ex):\n",
    "    q = str(ex[\"question\"]).strip()\n",
    "    c = str(ex[\"context\"]).strip()\n",
    "    a = str(ex[\"answer\"]).strip()\n",
    "\n",
    "    # Prompt simple y consistente para SFT\n",
    "    text = f\"\"\"You are a professional assistant for international students at the University of North Texas (UNT).\n",
    "Always answer in the SAME language as the user's question (English question -> English answer, Spanish question -> Spanish answer).\n",
    "Be precise, specific, and factual. Use the context.\n",
    "\n",
    "[CONTEXT]\n",
    "{c}\n",
    "\n",
    "[QUESTION]\n",
    "{q}\n",
    "\n",
    "[ANSWER]\n",
    "{a}\n",
    "\"\"\"\n",
    "    return {\"text\": text}\n",
    "\n",
    "train_ds = train_raw.map(build_text)\n",
    "test_ds  = test_raw.map(build_text)\n",
    "\n",
    "print(train_ds[0][\"text\"][:700])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3741585-16ef-44b9-9267-b16c06131d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU count: 2\n",
      "0 NVIDIA RTX A4500\n",
      "1 NVIDIA RTX A4500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55fe5ebbde014e1bba8ab39d8be2a049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model with device_map='auto'.\n",
      "First param device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Cell 6 (REPLACED): Load tokenizer + base model sharded across GPU0+GPU1\n",
    "\n",
    "import os, torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU count:\", torch.cuda.device_count())\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(i, torch.cuda.get_device_name(i))\n",
    "\n",
    "# IMPORTANT: allow both GPUs (if your env restricts GPUs, set it here)\n",
    "# If you know you have GPU:0 and GPU:1, ensure they are visible:\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"  # uncomment ONLY if needed before importing torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load sharded model (model parallel)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",              # âœ… spreads layers across available GPUs\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "# Sync embeddings\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Reduce VRAM usage\n",
    "model.gradient_checkpointing_enable()\n",
    "model.config.use_cache = False\n",
    "\n",
    "model.train()\n",
    "\n",
    "print(\"Loaded model with device_map='auto'.\")\n",
    "print(\"First param device:\", next(model.parameters()).device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a91ac481-6e7d-4109-8ab3-c47d2d792b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 2,097,152 / Total: 6,740,512,768 (0.0311%)\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Apply LoRA\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=4,                 # mÃ¡s pequeÃ±o para que quepa\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.train()\n",
    "\n",
    "# sanity: debe haber trainables\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable: {trainable:,} / Total: {total:,} ({100*trainable/total:.4f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75463e18-3566-482c-9890-9b054567912f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cabbe83b506d41189342e72652a9d33b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/540 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a97aff1bff547e6917cdfce425f171f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/61 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1461d496738c44fb95fd238e03442dce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/540 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c39e6f58f86e4c23954ebe189da71f32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/61 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 540\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 61\n",
       " }))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 8: Tokenize datasets\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LEN,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "train_tok = train_ds.map(tokenize_fn, batched=True, remove_columns=train_ds.column_names)\n",
    "test_tok  = test_ds.map(tokenize_fn, batched=True, remove_columns=test_ds.column_names)\n",
    "\n",
    "# Labels para causal LM\n",
    "train_tok = train_tok.map(lambda x: {\"labels\": x[\"input_ids\"]})\n",
    "test_tok  = test_tok.map(lambda x: {\"labels\": x[\"input_ids\"]})\n",
    "\n",
    "train_tok, test_tok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae274ef1-9f81-4fd1-aa5e-a8576fd6024a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-27 14:14:30.737299: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/STUDENTS/hel0057/.local/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [31/32 05:18 < 00:10, 0.09 it/s, Epoch 1.78/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.383100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 9: Train with Trainer (Adafactor to reduce optimizer memory)\n",
    "\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "bf16_ok = False\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        bf16_ok = torch.cuda.is_bf16_supported()\n",
    "    except Exception:\n",
    "        bf16_ok = False\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACC,\n",
    "    learning_rate=LR,\n",
    "    fp16=False,\n",
    "    bf16 = bf16_ok,\n",
    "    logging_steps=25,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"no\",\n",
    "    report_to=\"none\",\n",
    "    seed=SEED,\n",
    "    optim=\"adafactor\",\n",
    "    # âœ… clave cuando usas device_map model parallel:\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(\"Saved to:\", OUTPUT_DIR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f9bea8-7729-4db4-ab65-ff65d689fcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Build LangChain Documents from CSV (context + answer)\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "docs = []\n",
    "for i, row in df.iterrows():\n",
    "    q = str(row[\"question\"]).strip()\n",
    "    c = str(row[\"context\"]).strip()\n",
    "    a = str(row[\"answer\"]).strip()\n",
    "\n",
    "    content = f\"CONTEXT:\\n{c}\\n\\nREFERENCE_ANSWER:\\n{a}\"\n",
    "    docs.append(Document(page_content=content, metadata={\"row_id\": int(i), \"question\": q}))\n",
    "\n",
    "print(\"Docs:\", len(docs))\n",
    "print(docs[0].page_content[:400])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff463922-2fb3-4f7c-8af8-c319f8f1b9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: FAISS vectorstore (CPU embeddings to avoid any CUDA issues)\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    model_kwargs={\"device\": \"cpu\"}   # âœ… force CPU for embedding (retrieval won't touch CUDA)\n",
    ")\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "FAISS_DIR = \"faiss_unt_index_llama2\"\n",
    "vectorstore.save_local(FAISS_DIR)\n",
    "\n",
    "print(\"âœ… FAISS saved to:\", FAISS_DIR)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0150b758-8bac-4427-a87b-4fddba87b1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Load fine-tuned LoRA model for inference (device_map auto) + generate_text()\n",
    "\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(OUTPUT_DIR, use_fast=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "# Use BF16 if supported, else FP16 (inference)\n",
    "dtype = torch.float16\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        if torch.cuda.is_bf16_supported():\n",
    "            dtype = torch.bfloat16\n",
    "    except Exception:\n",
    "        pass\n",
    "else:\n",
    "    dtype = torch.float32\n",
    "\n",
    "print(\"Inference dtype:\", dtype)\n",
    "\n",
    "# Load sharded base model (uses GPU0+GPU1 if available)\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=dtype,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "base.resize_token_embeddings(len(tok))\n",
    "base.config.use_cache = True\n",
    "\n",
    "# Attach LoRA adapters\n",
    "ft = PeftModel.from_pretrained(base, OUTPUT_DIR)\n",
    "ft.eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_text(\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 256,\n",
    "    temperature: float = 0.3,\n",
    "    top_p: float = 0.9,\n",
    "    repetition_penalty: float = 1.1,\n",
    "    max_prompt_tokens: int = 512\n",
    ") -> str:\n",
    "    inputs = tok(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_prompt_tokens,\n",
    "        padding=False\n",
    "    )\n",
    "\n",
    "    # For sharded models, move inputs to the first device\n",
    "    first_device = next(ft.parameters()).device\n",
    "    inputs = {k: v.to(first_device) for k, v in inputs.items()}\n",
    "\n",
    "    out = ft.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs.get(\"attention_mask\", None),\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        pad_token_id=tok.eos_token_id,\n",
    "        eos_token_id=tok.eos_token_id\n",
    "    )\n",
    "\n",
    "    full = tok.decode(out[0], skip_special_tokens=True)\n",
    "    if full.startswith(prompt):\n",
    "        return full[len(prompt):].strip()\n",
    "    return full.strip()\n",
    "\n",
    "print(\"âœ… Inference ready (generate_text available).\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103301d6-c44e-42ee-accc-fd99bec13b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: LangChain RAG chain (RetrievalQA) using generate_text()\n",
    "\n",
    "from typing import Any, List, Optional\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "class LocalLLM(LLM):\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"local_llama2_generate\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None, **kwargs: Any) -> str:\n",
    "        text = generate_text(prompt)\n",
    "        if stop:\n",
    "            for s in stop:\n",
    "                if s in text:\n",
    "                    text = text.split(s)[0]\n",
    "        return text\n",
    "\n",
    "llm = LocalLLM()\n",
    "\n",
    "rag_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"You are a professional assistant for international students at the University of North Texas (UNT).\n",
    "Always answer in the SAME language as the user's question (English question -> English answer, Spanish question -> Spanish answer).\n",
    "Use ONLY the provided context. If the context does not contain the answer, say what is missing and what the student should check next.\n",
    "\n",
    "[RETRIEVED CONTEXT]\n",
    "{context}\n",
    "\n",
    "[USER QUESTION]\n",
    "{question}\n",
    "\n",
    "[FINAL ANSWER]\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": rag_prompt},\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "def chat_rag(question: str, k: int = 3):\n",
    "    qa_chain.retriever.search_kwargs[\"k\"] = k\n",
    "    res = qa_chain({\"query\": question})\n",
    "    ans = res[\"result\"]\n",
    "    srcs = res.get(\"source_documents\", [])\n",
    "    return ans, [(d.metadata.get(\"row_id\"), d.metadata.get(\"question\")) for d in srcs]\n",
    "\n",
    "print(\"âœ… RAG chain ready (chat_rag available).\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891269a8-7a20-4839-a7d2-c8bb5b036e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Quick tests\n",
    "\n",
    "ans_es, src_es = chat_rag(\"Â¿CuÃ¡l es el proceso para obtener el I-20 despuÃ©s de ser admitido?\", k=3)\n",
    "print(\"---- RAG ES ----\")\n",
    "print(ans_es)\n",
    "print(\"\\nSources:\")\n",
    "for rid, qq in src_es:\n",
    "    print(f\"- {rid}: {qq}\")\n",
    "\n",
    "ans_en, src_en = chat_rag(\"What are typical housing options for international graduate students at UNT?\", k=3)\n",
    "print(\"\\n---- RAG EN ----\")\n",
    "print(ans_en)\n",
    "print(\"\\nSources:\")\n",
    "for rid, qq in src_en:\n",
    "    print(f\"- {rid}: {qq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a331d435-e524-47ed-8c51-bdb986efac4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
