{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98b7818a-389d-466d-a162-3cff1fb44b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned. CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import os, gc, torch\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Cleaned. CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5971e4b-93ec-4d11-8cf2-72ee43b1dd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Paste your HF token (input hidden):  Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_TOKEN set: True\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Set HF token safely (do NOT print it)\n",
    "\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if \"HF_TOKEN\" not in os.environ:\n",
    "    os.environ[\"HF_TOKEN\"] = getpass(\"Paste your HF token (input hidden): \")\n",
    "\n",
    "print(\"HF_TOKEN set:\", \"HF_TOKEN\" in os.environ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "430383ee-2160-4571-a93a-e7d20fe176d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/STUDENTS/hel0057/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:979: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f699c389ae074686a2393d80ab9aa2f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 16 files:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded to: ./local_llama2_model\n"
     ]
    }
   ],
   "source": [
    "# Cell 2b: snapshot_download Llama-2-7b-chat-hf to local dir\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "LOCAL_MODEL_DIR = \"./local_llama2_model\"\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    local_dir=LOCAL_MODEL_DIR,\n",
    "    token=os.environ[\"HF_TOKEN\"],\n",
    "    local_dir_use_symlinks=False\n",
    ")\n",
    "\n",
    "print(\"Downloaded to:\", LOCAL_MODEL_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27c79dd9-bc39-49dc-899e-13ea96ac89e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7b7bc1bef690>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 3: Config\n",
    "\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "BASE_MODEL = LOCAL_MODEL_DIR\n",
    "OUTPUT_DIR = \"llama2_7b_unt_lora_rag\"\n",
    "\n",
    "# Training params (agresivos para que quepa en 20GB)\n",
    "MAX_SEQ_LEN = 192\n",
    "BATCH_SIZE = 1\n",
    "GRAD_ACC = 32\n",
    "EPOCHS = 2\n",
    "LR = 2e-4\n",
    "SEED = 42\n",
    "\n",
    "torch.manual_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0af5c1f-da53-48d6-840b-dc6180b7f263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 601\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['question', 'context', 'answer'],\n",
       "     num_rows: 540\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['question', 'context', 'answer'],\n",
       "     num_rows: 61\n",
       " }))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 4: Load CSV and split\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "csv_path = \"qa_dataset.csv\"  # tu archivo\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "required = {\"question\", \"context\", \"answer\"}\n",
    "missing = required - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns: {missing}\")\n",
    "\n",
    "df = df.dropna(subset=[\"question\", \"context\", \"answer\"]).reset_index(drop=True)\n",
    "print(\"Rows:\", len(df))\n",
    "\n",
    "ds = Dataset.from_pandas(df)\n",
    "split = ds.train_test_split(test_size=0.1, seed=SEED)\n",
    "train_raw, test_raw = split[\"train\"], split[\"test\"]\n",
    "train_raw, test_raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9c9a538-822a-4dc1-ae65-b06cbfa6e2f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12348ed867394e2a8f6f98e1980735ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/540 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "881ae0209d724b5da67445e8f606ae14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/61 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a professional assistant for international students at the University of North Texas (UNT).\n",
      "Always answer in the SAME language as the user's question (English question -> English answer, Spanish question -> Spanish answer).\n",
      "Be precise, specific, and factual. Use the context.\n",
      "\n",
      "[CONTEXT]\n",
      "If you have trouble, don't hesitate to ask for help. The International Student Office is your primary resource. They can give you advice on cultural adjustment, visa rules, or refer you to academic and mental health counselors on campus. Universities have many resources to help you succeed.\n",
      "\n",
      "[QUESTION]\n",
      "What should I do if I have trouble adapting?\n",
      "\n",
      "[ANSWER]\n",
      "If you're struggling, contact the Internationa\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Build SFT text field\n",
    "\n",
    "def build_text(ex):\n",
    "    q = str(ex[\"question\"]).strip()\n",
    "    c = str(ex[\"context\"]).strip()\n",
    "    a = str(ex[\"answer\"]).strip()\n",
    "\n",
    "    # Prompt simple y consistente para SFT\n",
    "    text = f\"\"\"You are a professional assistant for international students at the University of North Texas (UNT).\n",
    "Always answer in the SAME language as the user's question (English question -> English answer, Spanish question -> Spanish answer).\n",
    "Be precise, specific, and factual. Use the context.\n",
    "\n",
    "[CONTEXT]\n",
    "{c}\n",
    "\n",
    "[QUESTION]\n",
    "{q}\n",
    "\n",
    "[ANSWER]\n",
    "{a}\n",
    "\"\"\"\n",
    "    return {\"text\": text}\n",
    "\n",
    "train_ds = train_raw.map(build_text)\n",
    "test_ds  = test_raw.map(build_text)\n",
    "\n",
    "print(train_ds[0][\"text\"][:700])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3741585-16ef-44b9-9267-b16c06131d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU count: 2\n",
      "0 NVIDIA RTX A4500\n",
      "1 NVIDIA RTX A4500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bbaa0673931417b9d5f8e588de42d05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model with device_map='auto'.\n",
      "First param device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Cell 6 (REPLACED): Load tokenizer + base model sharded across GPU0+GPU1\n",
    "\n",
    "import os, torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU count:\", torch.cuda.device_count())\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(i, torch.cuda.get_device_name(i))\n",
    "\n",
    "# IMPORTANT: allow both GPUs (if your env restricts GPUs, set it here)\n",
    "# If you know you have GPU:0 and GPU:1, ensure they are visible:\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"  # uncomment ONLY if needed before importing torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load sharded model (model parallel)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",              # âœ… spreads layers across available GPUs\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "# Sync embeddings\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Reduce VRAM usage\n",
    "model.gradient_checkpointing_enable()\n",
    "model.config.use_cache = False\n",
    "\n",
    "model.train()\n",
    "\n",
    "print(\"Loaded model with device_map='auto'.\")\n",
    "print(\"First param device:\", next(model.parameters()).device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a91ac481-6e7d-4109-8ab3-c47d2d792b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 2,097,152 / Total: 6,740,512,768 (0.0311%)\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Apply LoRA\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=4,                 # mÃ¡s pequeÃ±o para que quepa\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.train()\n",
    "\n",
    "# sanity: debe haber trainables\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable: {trainable:,} / Total: {total:,} ({100*trainable/total:.4f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75463e18-3566-482c-9890-9b054567912f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fba1c1c0cc4f4e72baf0be0c40af9f4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/540 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d0298bdb16a4a6a9353355b44c2d7a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/61 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2aecc13eb6c4c56ba96b158ff86bd45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/540 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4982cd379b084206aed556124a04f567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/61 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 540\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 61\n",
       " }))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 8: Tokenize datasets\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LEN,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "train_tok = train_ds.map(tokenize_fn, batched=True, remove_columns=train_ds.column_names)\n",
    "test_tok  = test_ds.map(tokenize_fn, batched=True, remove_columns=test_ds.column_names)\n",
    "\n",
    "# Labels para causal LM\n",
    "train_tok = train_tok.map(lambda x: {\"labels\": x[\"input_ids\"]})\n",
    "test_tok  = test_tok.map(lambda x: {\"labels\": x[\"input_ids\"]})\n",
    "\n",
    "train_tok, test_tok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae274ef1-9f81-4fd1-aa5e-a8576fd6024a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-27 12:23:43.296034: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/STUDENTS/hel0057/.local/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 05:44, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.383700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: llama2_7b_unt_lora_rag\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/STUDENTS/hel0057/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./local_llama2_model - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Train with Trainer (Adafactor to reduce optimizer memory)\n",
    "\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "bf16_ok = False\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        bf16_ok = torch.cuda.is_bf16_supported()\n",
    "    except Exception:\n",
    "        bf16_ok = False\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACC,\n",
    "    learning_rate=LR,\n",
    "    fp16=False,\n",
    "    bf16 = bf16_ok,\n",
    "    logging_steps=25,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"no\",\n",
    "    report_to=\"none\",\n",
    "    seed=SEED,\n",
    "    optim=\"adafactor\",\n",
    "    # âœ… clave cuando usas device_map model parallel:\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(\"Saved to:\", OUTPUT_DIR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56f9bea8-7729-4db4-ab65-ff65d689fcef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs: 601\n",
      "CONTEXT:\n",
      "OPT is a temporary employment that is directly related to an F-1 studentâ€™s major area of study.\n",
      "\n",
      "REFERENCE_ANSWER:\n",
      "OPT allows international students to work in their field of study for up to 12 months after graduation.\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Build LangChain Documents from CSV (context + answer)\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "docs = []\n",
    "for i, row in df.iterrows():\n",
    "    q = str(row[\"question\"]).strip()\n",
    "    c = str(row[\"context\"]).strip()\n",
    "    a = str(row[\"answer\"]).strip()\n",
    "\n",
    "    content = f\"CONTEXT:\\n{c}\\n\\nREFERENCE_ANSWER:\\n{a}\"\n",
    "    docs.append(Document(page_content=content, metadata={\"row_id\": int(i), \"question\": q}))\n",
    "\n",
    "print(\"Docs:\", len(docs))\n",
    "print(docs[0].page_content[:400])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff463922-2fb3-4f7c-8af8-c319f8f1b9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/STUDENTS/hel0057/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… FAISS saved to: faiss_unt_index_llama2\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: FAISS vectorstore (CPU embeddings to avoid any CUDA issues)\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    model_kwargs={\"device\": \"cpu\"}   # âœ… force CPU for embedding (retrieval won't touch CUDA)\n",
    ")\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "FAISS_DIR = \"faiss_unt_index_llama2\"\n",
    "vectorstore.save_local(FAISS_DIR)\n",
    "\n",
    "print(\"âœ… FAISS saved to:\", FAISS_DIR)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0150b758-8bac-4427-a87b-4fddba87b1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference dtype: torch.bfloat16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "126dabfd21804f01896c8adf81cd890f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "weight is on the meta device, we need a `value` to put in on 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m base\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Attach LoRA adapters\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m ft \u001b[38;5;241m=\u001b[39m \u001b[43mPeftModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m ft\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_text\u001b[39m(\n\u001b[1;32m     41\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m     max_prompt_tokens: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m\n\u001b[1;32m     47\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/peft/peft_model.py:430\u001b[0m, in \u001b[0;36mPeftModel.from_pretrained\u001b[0;34m(cls, model, model_id, adapter_name, is_trainable, config, **kwargs)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    429\u001b[0m     model \u001b[38;5;241m=\u001b[39m MODEL_TYPE_TO_PEFT_MODEL_MAPPING[config\u001b[38;5;241m.\u001b[39mtask_type](model, config, adapter_name)\n\u001b[0;32m--> 430\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_trainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_trainable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/peft/peft_model.py:1025\u001b[0m, in \u001b[0;36mPeftModel.load_adapter\u001b[0;34m(self, model_id, adapter_name, is_trainable, torch_device, **kwargs)\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_offload(offload_index, adapters_weights)\n\u001b[1;32m   1023\u001b[0m dispatch_model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffload_index\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m offload_index\n\u001b[0;32m-> 1025\u001b[0m \u001b[43mdispatch_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffload_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdispatch_model_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1032\u001b[0m hook \u001b[38;5;241m=\u001b[39m AlignDevicesHook(io_same_device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config[adapter_name]\u001b[38;5;241m.\u001b[39mis_prompt_learning:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/big_modeling.py:419\u001b[0m, in \u001b[0;36mdispatch_model\u001b[0;34m(model, device_map, main_device, state_dict, offload_dir, offload_index, offload_buffers, skip_keys, preload_module_classes, force_hooks)\u001b[0m\n\u001b[1;32m    414\u001b[0m         tied_params_map[data_ptr] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    416\u001b[0m         \u001b[38;5;66;03m# Note: To handle the disk offloading case, we can not simply use weights_map[param_name].data_ptr() as the reference pointer,\u001b[39;00m\n\u001b[1;32m    417\u001b[0m         \u001b[38;5;66;03m# as we have no guarantee that safetensors' `file.get_tensor()` will always give the same pointer.\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m \u001b[43mattach_align_device_hook_on_blocks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecution_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecution_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffload_buffers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_buffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweights_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_module_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_module_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtied_params_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtied_params_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# warn if there is any params on the meta device\u001b[39;00m\n\u001b[1;32m    431\u001b[0m offloaded_devices_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m    432\u001b[0m     [device \u001b[38;5;28;01mfor\u001b[39;00m device \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(device_map\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m    433\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py:648\u001b[0m, in \u001b[0;36mattach_align_device_hook_on_blocks\u001b[0;34m(module, execution_device, offload, weights_map, offload_buffers, module_name, skip_keys, preload_module_classes, tied_params_map)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child_name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_children():\n\u001b[1;32m    647\u001b[0m     child_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchild_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(module_name) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m child_name\n\u001b[0;32m--> 648\u001b[0m     \u001b[43mattach_align_device_hook_on_blocks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexecution_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecution_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_buffers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_buffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchild_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_module_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_module_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtied_params_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtied_params_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py:648\u001b[0m, in \u001b[0;36mattach_align_device_hook_on_blocks\u001b[0;34m(module, execution_device, offload, weights_map, offload_buffers, module_name, skip_keys, preload_module_classes, tied_params_map)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child_name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_children():\n\u001b[1;32m    647\u001b[0m     child_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchild_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(module_name) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m child_name\n\u001b[0;32m--> 648\u001b[0m     \u001b[43mattach_align_device_hook_on_blocks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexecution_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecution_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_buffers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_buffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchild_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_module_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_module_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtied_params_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtied_params_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py:648\u001b[0m, in \u001b[0;36mattach_align_device_hook_on_blocks\u001b[0;34m(module, execution_device, offload, weights_map, offload_buffers, module_name, skip_keys, preload_module_classes, tied_params_map)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child_name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_children():\n\u001b[1;32m    647\u001b[0m     child_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchild_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(module_name) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m child_name\n\u001b[0;32m--> 648\u001b[0m     \u001b[43mattach_align_device_hook_on_blocks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexecution_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecution_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_buffers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_buffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchild_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_module_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_module_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtied_params_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtied_params_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py:608\u001b[0m, in \u001b[0;36mattach_align_device_hook_on_blocks\u001b[0;34m(module, execution_device, offload, weights_map, offload_buffers, module_name, skip_keys, preload_module_classes, tied_params_map)\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module_name \u001b[38;5;129;01min\u001b[39;00m execution_device \u001b[38;5;129;01mand\u001b[39;00m module_name \u001b[38;5;129;01min\u001b[39;00m offload \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m offload[module_name]:\n\u001b[1;32m    600\u001b[0m     hook \u001b[38;5;241m=\u001b[39m AlignDevicesHook(\n\u001b[1;32m    601\u001b[0m         execution_device\u001b[38;5;241m=\u001b[39mexecution_device[module_name],\n\u001b[1;32m    602\u001b[0m         offload_buffers\u001b[38;5;241m=\u001b[39moffload_buffers,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    606\u001b[0m         tied_params_map\u001b[38;5;241m=\u001b[39mtied_params_map,\n\u001b[1;32m    607\u001b[0m     )\n\u001b[0;32m--> 608\u001b[0m     \u001b[43madd_hook_to_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    609\u001b[0m     attach_execution_device_hook(module, execution_device[module_name], tied_params_map\u001b[38;5;241m=\u001b[39mtied_params_map)\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m module_name \u001b[38;5;129;01min\u001b[39;00m execution_device \u001b[38;5;129;01mand\u001b[39;00m module_name \u001b[38;5;129;01min\u001b[39;00m offload:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py:157\u001b[0m, in \u001b[0;36madd_hook_to_module\u001b[0;34m(module, hook, append)\u001b[0m\n\u001b[1;32m    154\u001b[0m     old_forward \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39mforward\n\u001b[1;32m    155\u001b[0m     module\u001b[38;5;241m.\u001b[39m_old_forward \u001b[38;5;241m=\u001b[39m old_forward\n\u001b[0;32m--> 157\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m module\u001b[38;5;241m.\u001b[39m_hf_hook \u001b[38;5;241m=\u001b[39m hook\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mnew_forward\u001b[39m(module, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py:275\u001b[0m, in \u001b[0;36mAlignDevicesHook.init_hook\u001b[0;34m(self, module)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffload \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, _ \u001b[38;5;129;01min\u001b[39;00m named_module_tensors(module, recurse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplace_submodules):\n\u001b[0;32m--> 275\u001b[0m         \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtied_params_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtied_params_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffload:\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moriginal_devices \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    278\u001b[0m         name: param\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m named_module_tensors(module, recurse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplace_submodules)\n\u001b[1;32m    279\u001b[0m     }\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/utils/modeling.py:354\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m old_value\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m device \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m)] \u001b[38;5;129;01mand\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 354\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is on the meta device, we need a `value` to put in on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m old_value\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m value\u001b[38;5;241m.\u001b[39mshape:\n",
      "\u001b[0;31mValueError\u001b[0m: weight is on the meta device, we need a `value` to put in on 1."
     ]
    }
   ],
   "source": [
    "# Cell 12: Load fine-tuned LoRA model for inference (device_map auto) + generate_text()\n",
    "\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(OUTPUT_DIR, use_fast=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "# Use BF16 if supported, else FP16 (inference)\n",
    "dtype = torch.float16\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        if torch.cuda.is_bf16_supported():\n",
    "            dtype = torch.bfloat16\n",
    "    except Exception:\n",
    "        pass\n",
    "else:\n",
    "    dtype = torch.float32\n",
    "\n",
    "print(\"Inference dtype:\", dtype)\n",
    "\n",
    "# Load sharded base model (uses GPU0+GPU1 if available)\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=dtype,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "base.resize_token_embeddings(len(tok))\n",
    "base.config.use_cache = True\n",
    "\n",
    "# Attach LoRA adapters\n",
    "ft = PeftModel.from_pretrained(base, OUTPUT_DIR)\n",
    "ft.eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_text(\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 256,\n",
    "    temperature: float = 0.3,\n",
    "    top_p: float = 0.9,\n",
    "    repetition_penalty: float = 1.1,\n",
    "    max_prompt_tokens: int = 512\n",
    ") -> str:\n",
    "    inputs = tok(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_prompt_tokens,\n",
    "        padding=False\n",
    "    )\n",
    "\n",
    "    # For sharded models, move inputs to the first device\n",
    "    first_device = next(ft.parameters()).device\n",
    "    inputs = {k: v.to(first_device) for k, v in inputs.items()}\n",
    "\n",
    "    out = ft.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs.get(\"attention_mask\", None),\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        pad_token_id=tok.eos_token_id,\n",
    "        eos_token_id=tok.eos_token_id\n",
    "    )\n",
    "\n",
    "    full = tok.decode(out[0], skip_special_tokens=True)\n",
    "    if full.startswith(prompt):\n",
    "        return full[len(prompt):].strip()\n",
    "    return full.strip()\n",
    "\n",
    "print(\"âœ… Inference ready (generate_text available).\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "103301d6-c44e-42ee-accc-fd99bec13b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… RAG chain ready (chat_rag available).\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: LangChain RAG chain (RetrievalQA) using generate_text()\n",
    "\n",
    "from typing import Any, List, Optional\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "class LocalLLM(LLM):\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"local_llama2_generate\"\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None, **kwargs: Any) -> str:\n",
    "        text = generate_text(prompt)\n",
    "        if stop:\n",
    "            for s in stop:\n",
    "                if s in text:\n",
    "                    text = text.split(s)[0]\n",
    "        return text\n",
    "\n",
    "llm = LocalLLM()\n",
    "\n",
    "rag_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"You are a professional assistant for international students at the University of North Texas (UNT).\n",
    "Always answer in the SAME language as the user's question (English question -> English answer, Spanish question -> Spanish answer).\n",
    "Use ONLY the provided context. If the context does not contain the answer, say what is missing and what the student should check next.\n",
    "\n",
    "[RETRIEVED CONTEXT]\n",
    "{context}\n",
    "\n",
    "[USER QUESTION]\n",
    "{question}\n",
    "\n",
    "[FINAL ANSWER]\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": rag_prompt},\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "def chat_rag(question: str, k: int = 3):\n",
    "    qa_chain.retriever.search_kwargs[\"k\"] = k\n",
    "    res = qa_chain({\"query\": question})\n",
    "    ans = res[\"result\"]\n",
    "    srcs = res.get(\"source_documents\", [])\n",
    "    return ans, [(d.metadata.get(\"row_id\"), d.metadata.get(\"question\")) for d in srcs]\n",
    "\n",
    "print(\"âœ… RAG chain ready (chat_rag available).\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891269a8-7a20-4839-a7d2-c8bb5b036e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Quick tests\n",
    "\n",
    "ans_es, src_es = chat_rag(\"Â¿CuÃ¡l es el proceso para obtener el I-20 despuÃ©s de ser admitido?\", k=3)\n",
    "print(\"---- RAG ES ----\")\n",
    "print(ans_es)\n",
    "print(\"\\nSources:\")\n",
    "for rid, qq in src_es:\n",
    "    print(f\"- {rid}: {qq}\")\n",
    "\n",
    "ans_en, src_en = chat_rag(\"What are typical housing options for international graduate students at UNT?\", k=3)\n",
    "print(\"\\n---- RAG EN ----\")\n",
    "print(ans_en)\n",
    "print(\"\\nSources:\")\n",
    "for rid, qq in src_en:\n",
    "    print(f\"- {rid}: {qq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a331d435-e524-47ed-8c51-bdb986efac4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
